{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCTM for NeurIPS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfb = tfp.bijectors\n",
    "tfd = tfp.distributions\n",
    "tfk = tfp.math.psd_kernels\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "from imp import reload\n",
    "from scipy import sparse as sp\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "\n",
    "import correlated_topic_model as ctmd\n",
    "import dynamic_correlated_topic_model as dctm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data with:\n",
    "!curl https://archive.ics.uci.edu/ml/machine-learning-databases/00371/NIPS_1987-2015.csv -o NIPS_1987-2015.csv\n",
    "# or \n",
    "# !wget https://archive.ics.uci.edu/ml/machine-learning-databases/00371/NIPS_1987-2015.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may need the following\n",
    "# import nltk\n",
    "# nltk.download('words')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "import datasets\n",
    "\n",
    "df, years, vocabulary = datasets.get_neurips('NIPS_1987-2015.csv')\n",
    "\n",
    "vocabulary_subset = vocabulary[vocabulary > 1700].index\n",
    "\n",
    "X_small = df.loc[vocabulary_subset].T.dropna()\n",
    "X_small = X_small.loc[X_small.sum(axis=1) > 0]\n",
    "\n",
    "year = np.array([x.split('_')[0] for x in X_small.index])\n",
    "X = np.expand_dims(X_small.values.astype(np.float64), -2)\n",
    "\n",
    "import sklearn, pandas as pd\n",
    "scaler = sklearn.preprocessing.MinMaxScaler([-1, 1])\n",
    "index_points = scaler.fit_transform(year.astype(int)[:, None])\n",
    "# index_points = year.astype(np.float64)[:, None]\n",
    "\n",
    "np.random.seed(42)\n",
    "(X_tr, X_ts, index_tr, index_ts, X_tr_sorted, X_ts_sorted,\n",
    " index_tr_sorted, index_ts_sorted\n",
    ") = datasets.train_test_split(X, index_points)\n",
    "\n",
    "inverse_transform_fn = lambda x: pd.to_datetime(scaler.inverse_transform(x)[:, 0], format='%Y')\n",
    "df_train = pd.DataFrame(X_tr_sorted[:, 0, :])\n",
    "df_train['years'] = inverse_transform_fn(index_tr_sorted)\n",
    "\n",
    "df_test = pd.DataFrame(X_ts_sorted[:, 0, :])\n",
    "df_test['years'] = inverse_transform_fn(index_ts_sorted)\n",
    "\n",
    "print(\"Dataset shape: \\n tr: {} \\n ts: {}\".format(X_tr.shape, X_ts.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_train_samples = X_tr.shape[0]\n",
    "\n",
    "dataset = tf.data.Dataset.zip(\n",
    "    tuple(map(tf.data.Dataset.from_tensor_slices,\n",
    "        (X_tr, index_tr))))\n",
    "dataset = dataset.shuffle(n_train_samples, reshuffle_each_iteration=True)\n",
    "data_tr = dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inducing_index_points_beta = np.linspace(-1, 1, 15)[:, None]\n",
    "inducing_index_points_mu = np.linspace(-1, 1, 20)[:, None]\n",
    "inducing_index_points_ell = np.linspace(-1, 1, 15)[:, None]\n",
    "\n",
    "dtype = np.float64\n",
    "amplitude_beta = tfp.util.TransformedVariable(\n",
    "    1., bijector=tfb.Softplus(), dtype=dtype, name='amplitude_beta')\n",
    "length_scale_beta = tfp.util.TransformedVariable(\n",
    "    0.5, bijector=tfb.Softplus(), dtype=dtype,\n",
    "    name='length_scale_beta')\n",
    "kernel_beta = tfk.MaternOneHalf(amplitude=amplitude_beta, length_scale=length_scale_beta)\n",
    "\n",
    "amplitude_mu = tfp.util.TransformedVariable(\n",
    "    1., bijector=tfb.Softplus(), dtype=dtype, name=\"amplitude_mu\")\n",
    "length_scale_mu = tfp.util.TransformedVariable(\n",
    "    0.5, bijector=tfb.Softplus(), dtype=dtype,\n",
    "    name=\"length_scale_mu\")\n",
    "kernel_mu = tfk.ExponentiatedQuadratic(amplitude=amplitude_mu, length_scale=length_scale_mu)\n",
    "\n",
    "amplitude_ell = tfp.util.TransformedVariable(\n",
    "    1., bijector=tfb.Softplus(), dtype=dtype, name='amplitude_ell')\n",
    "length_scale_ell = tfp.util.TransformedVariable(\n",
    "    0.5, bijector=tfb.Softplus(), dtype=dtype,\n",
    "    name='length_scale_ell')\n",
    "kernel_ell = tfk.ExponentiatedQuadratic(amplitude=amplitude_ell, length_scale=length_scale_ell)\n",
    "\n",
    "reload(ctmd)\n",
    "reload(dctm);\n",
    "\n",
    "losses = []\n",
    "perplexities = []\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "mdl = dctm.DCTM(\n",
    "    n_topics=30, n_words=vocabulary_subset.size,\n",
    "    kernel_beta=kernel_beta,\n",
    "    index_points_beta=np.unique(index_tr)[:, None],\n",
    "    inducing_index_points_beta=inducing_index_points_beta,\n",
    "    kernel_ell=kernel_ell,\n",
    "    kernel_mu=kernel_mu,\n",
    "    index_points_mu=np.unique(index_tr)[:, None],\n",
    "    index_points_ell=np.unique(index_tr)[:, None],\n",
    "    inducing_index_points_mu=inducing_index_points_mu,\n",
    "    inducing_index_points_ell=inducing_index_points_ell,\n",
    "    layer_sizes=(500, 300, 200),\n",
    "    jitter_beta=1e-6,\n",
    "    jitter_mu=1e-5, \n",
    "    jitter_ell=1e-6,\n",
    "    encoder_jitter=1e-8,dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_iter = 2\n",
    "pbar = tqdm(range(n_iter), disable=False)\n",
    "\n",
    "with tf.device('gpu'): \n",
    "    for epoch in pbar:\n",
    "        loss_value = 0\n",
    "        perplexity_value = 0\n",
    "\n",
    "        for x_batch, index_points_batch in data_tr:\n",
    "            loss, perpl = mdl.batch_optimize(\n",
    "                x_batch,\n",
    "                optimizer=optimizer,\n",
    "                observation_index_points=index_points_batch,\n",
    "                trainable_variables=None,\n",
    "                kl_weight=float(x_batch.shape[0]) / float(n_train_samples))\n",
    "            loss = tf.reduce_mean(loss, 0)\n",
    "            loss_value += loss\n",
    "            perplexity_value += perpl\n",
    "        pbar.set_description(\n",
    "            'loss {:.3e}, perpl {:.3e}'.format(loss_value, perplexity_value))\n",
    "\n",
    "        losses.append(loss_value)\n",
    "        perplexities.append(perplexity_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.semilogy();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(perplexities)\n",
    "plt.semilogy();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('gpu'):\n",
    "    elbo = mdl.elbo(X_ts, index_ts, kl_weight=0.)\n",
    "    perpl = mdl.perplexity(X_ts, elbo)\n",
    "    print(perpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl.n_topics = mdl.surrogate_posterior_beta.batch_shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inverse_transform_fn = lambda x: pd.to_datetime(scaler.inverse_transform(x)[:, 0], format='%Y').strftime('%Y')\n",
    "\n",
    "reload(dctm)\n",
    "tops = dctm.print_topics(\n",
    "    mdl, index_points=index_tr, vocabulary=vocabulary_subset,\n",
    "    inverse_transform_fn=inverse_transform_fn, top_n_topic=30, top_n_time=5)\n",
    "topics = np.array(tops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = mdl.surrogate_posterior_beta.batch_shape[-1]\n",
    "colors = plt.cm.jet(np.linspace(0, 1, n_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_points = np.linspace(-1,1, 100)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_sample, Sigma_sample = dctm.get_correlation(mdl.surrogate_posterior_ell.sample(1200, index_points=test_points))\n",
    "corr_10p = tfp.stats.percentile(corr_sample, 5, axis=0)\n",
    "corr = tfp.stats.percentile(corr_sample, 50, axis=0)\n",
    "corr_90p = tfp.stats.percentile(corr_sample, 95, axis=0)\n",
    "Sigma_10p = tfp.stats.percentile(Sigma_sample, 5, axis=0)\n",
    "Sigma = tfp.stats.percentile(Sigma_sample, 50, axis=0)\n",
    "Sigma_90p = tfp.stats.percentile(Sigma_sample, 95, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotting\n",
    "\n",
    "reload(plotting)\n",
    "plotting.plot_sigma(corr_sample, test_points, 11,\n",
    "    topics,\n",
    "    inverse_transform_fn,\n",
    "    restrict_to=None,\n",
    "    color_fn=plt.cm.tab20c,\n",
    "    legend='right', plot_if_higher_of=0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = mdl.predict(X).numpy()\n",
    "tmp_df = pd.DataFrame(topic[:,0,:], index=index_points[:, 0])\n",
    "topics_per_time = tmp_df.groupby(tmp_df.index).mean().values.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev = 0\n",
    "cm = plt.get_cmap('tab20c')\n",
    "colors = cm(np.linspace(0,1,9))\n",
    "\n",
    "topic_num = 11\n",
    "plt.title(\"Topic {}: {}\".format(topic_num, topics[topic_num][:35]))\n",
    "c = 0\n",
    "for t in range(n_topics):\n",
    "    if t == topic_num:# or t not in [13,19]:\n",
    "        continue\n",
    "    if tf.reduce_mean(np.abs(corr[:, topic_num, t])) < 0.15: continue\n",
    "    curr = prev+corr[:, topic_num, t]\n",
    "    plt.fill_between(test_points[:, 0],\n",
    "                     prev, curr, \n",
    "                     color=colors[c], label='{}:{}'.format(t, topics[t][:20]))\n",
    "    prev = curr\n",
    "    c += 1\n",
    "\n",
    "plt.xticks(test_points[::10], inverse_transform_fn(test_points)[::10], rotation=30);\n",
    "plt.gca().legend(loc='center left', bbox_to_anchor=(1, 0.5));\n",
    "f2 = plt.gcf()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev = 0\n",
    "cm = plt.get_cmap('tab20c')\n",
    "colors = cm(np.linspace(0,1,n_topics))\n",
    "\n",
    "topic_num = 19\n",
    "plt.title(\"Topic {}: {}\".format(topic_num, topics[topic_num][:35]))\n",
    "c = 0\n",
    "for t in range(n_topics):\n",
    "    if t == topic_num:# or t not in [13,19]:\n",
    "        continue\n",
    "#     if tf.reduce_mean(np.abs(corr[:, topic_num, t])) < 0.15:\n",
    "#         continue\n",
    "    curr = prev + corr[:, topic_num, t]\n",
    "    plt.fill_between(test_points[:, 0], prev, curr, \n",
    "                     color=colors[c], label='{}:{}'.format(t, topics[t][:20]))\n",
    "    prev = curr\n",
    "    c += 1\n",
    "\n",
    "plt.xticks(test_points[::10], inverse_transform_fn(test_points)[::10], rotation=30);\n",
    "#     plt.ylim([None,0.5])\n",
    "plt.gca().legend(loc='center left', bbox_to_anchor=(1, 0.5));\n",
    "f2 = plt.gcf()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_sigma(corr_sample, test_points, 19, topics, inverse_transform_fn, restrict_to=[2,5,11,12,14,15,19],legend='bottom');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f2.savefig('neurips_correlation_neuroscience_vertical.pdf', dpi=600, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_sigma(Sigma_sample, test_points, 15, topics, inverse_transform_fn, restrict_to=[13,19],legend='bottom');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f.savefig('class_correlation1.pdf', dpi=600, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a topic, let's show the correlation with the others. $\\Sigma$ with error bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for topic_num in range(n_topics):\n",
    "    plt.title(\"Topic {}: {}\".format(topic_num, topics[topic_num][:30]))\n",
    "    for t in range(n_topics):\n",
    "        if t == topic_num:# or t not in [0,1,2,15,3]:\n",
    "            continue\n",
    "        plt.plot(corr[:, topic_num, t], label='{}:{}'.format(t, topics[t][:20]), color=colors[t])\n",
    "\n",
    "    plt.xticks(range(test_points.size)[::10], inverse_transform_fn(test_points)[::10], rotation=45);\n",
    "    plt.xlim([20,None])\n",
    "    plt.gca().legend(loc='center left', bbox_to_anchor=(1, 0.5));\n",
    "    f = plt.gcf()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f.savefig('sample_correlation.pdf', dpi=600, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = mdl.predict(X)[:,0,:].numpy()\n",
    "tmp_df = pd.DataFrame(topic, index=index_points[:, 0])\n",
    "topics_per_time = tmp_df.groupby(tmp_df.index).mean().values.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(plotting)\n",
    "f = plotting.plot_predictions(\n",
    "    mdl, topics_per_time, index_points, topics, inverse_transform_fn,\n",
    "    restrict_to=None#[2,5,11,12,14,15,19]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(plotting)\n",
    "f = plotting.plot_predictions(\n",
    "    mdl, topics_per_time, index_points, topics, inverse_transform_fn,\n",
    "    restrict_to=[2,5,11,12,14,15,19],\n",
    "    legend='bottom'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f.savefig('neurips_topics_eta_vertical.pdf', dpi=600, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colors = plt.cm.jet(np.linspace(0,1,n_topics))\n",
    "mu = mdl.surrogate_posterior_mu.get_marginal_distribution(test_points)\n",
    "mu_sm = tf.nn.softmax(mu.mean(), axis=0)\n",
    "mu_sample = tf.nn.softmax(mu.sample(110), axis=1)\n",
    "mu_90p = tfp.stats.percentile(mu_sample, 95, axis=0)\n",
    "mu_10p = tfp.stats.percentile(mu_sample, 5, axis=0)\n",
    "\n",
    "for i in range(n_topics):\n",
    "    if tf.reduce_mean(tf.abs(mu_sm[i])) > 0.001:\n",
    "        line, = plt.plot(test_points, mu_sm[i], label=topics[i], color=colors[i]);\n",
    "        plt.fill_between(\n",
    "                test_points[:, 0],\n",
    "                mu_10p[i],\n",
    "                mu_90p[i],\n",
    "                color=line.get_color(),\n",
    "                alpha=0.3,\n",
    "                lw=1.5,\n",
    "            )\n",
    "\n",
    "        plt.plot(np.unique(index_points), topics_per_time[i], label='{}'.format(topics[i]), color=colors[i])\n",
    "\n",
    "        plt.xticks(test_points[::8], inverse_transform_fn(test_points)[::8], rotation=45);\n",
    "        plt.gca().legend(loc='center left', bbox_to_anchor=(1, 0.5));\n",
    "        plt.ylim(0,.3);\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability of topics over time.\n",
    "\n",
    "$\\mu$ with error bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plotting.plot_mu(\n",
    "    mdl, test_points, topics, inverse_transform_fn,\n",
    "    restrict_to=None, color_fn=lambda x:[None]*len(x), figsize=(9,5), plot_if_higher_of=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(plotting)\n",
    "# legends = [\n",
    "#     '2:layer unit hidder ar',\n",
    "#     '5:posterior bayesian g',\n",
    "#     '11:dirichlet topic expe',\n",
    "#     '12:theorem proof bound',\n",
    "#     '14:estim densiti sampl',\n",
    "#     '15:voltag channel signa',\n",
    "#     '19:neuron synapt fire c'\n",
    "# ]\n",
    "sample_size = 1\n",
    "f = plotting.plot_mu_stacked(\n",
    "    mean=tf.reduce_mean(\n",
    "        tf.nn.softmax(\n",
    "            tf.transpose(\n",
    "                tfd.MultivariateNormalTriL(\n",
    "                    loc=tfd.TransformedDistribution(\n",
    "                        tfd.Independent(mdl.surrogate_posterior_mu.get_marginal_distribution(test_points), 1),\n",
    "                        bijector=tfb.Transpose(rightmost_transposed_ndims=2),\n",
    "                    ).sample(sample_size),\n",
    "                    scale_tril=mdl.surrogate_posterior_ell.sample(sample_size, index_points=test_points),\n",
    "                ).sample()\n",
    "            ), axis=1),\n",
    "        -1),\n",
    "    test_points=test_points,\n",
    "    topics=topics,\n",
    "    inverse_transform_fn=inverse_transform_fn,\n",
    "    restrict_to=None, color_fn=plt.cm.tab20c, figsize=(9,5), plot_if_higher_of=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f.savefig('neurips_posterior_mu_vertical_new_2.pdf', dpi=600, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability of word-topic over time. $\\beta$ with error bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(plotting)\n",
    "with tf.device('CPU'):\n",
    "    f = plotting.plot_beta_and_stacked(\n",
    "        mdl, test_points, topic_num=1, vocabulary=vocabulary_subset, inverse_transform_fn=inverse_transform_fn,\n",
    "        topics=topics,\n",
    "        restrict_words_to=[\"lda\", \"topic\", \"document\", \"dirichlet\", \"hmm\", \"expert\", \"mixtur\", \"word\", \"latent\"],\n",
    "        figsize=(7,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f.savefig('neurips_posterior_beta_lda_vertical_2.pdf', dpi=600, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
